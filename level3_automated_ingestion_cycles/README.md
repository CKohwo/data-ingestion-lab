## ğŸ§  Level 3 â€“ Automated Ingestion cycles (scheduled data refresh)

This level marks the evolution from a simple HTML crawler into a autonomous ingestion system â€” capable of fetching data from different web sources (HTML scraping) and then intelligently produces a refreshing datasets on a fixed schedule.

The automation pipeline runs every 5 days via GitHub Actions & Render, it fetches and delivers data from multiple ingestion layers, and commits updates directly to this repository â€” creating a continuously self-refreshing data engine.

--------

## âš™ï¸ Core Concept

Objective:
Design a self-sustaining ingestion pipeline intelligently fetches data via web scraping (without APIs) into a distinct dataset.

At this stage, the system demonstrates true autonomy and resilience â€” updating itself without manual triggers, managing versioned data persistence, and maintaining a living dataset repository.

This is a key milestone toward a fully orchestrated ingestion engine (Level 5).

----------

## ğŸ§© Project Structure
``` bash
data-ingestion-lab/
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper_engine.py          # Reusable scraping logic (HTML)
â”‚    
â”‚
â”œâ”€â”€ sites/
â”‚   â”œâ”€â”€ categories.json            # Category mapping for scraping endpoints
â”‚   â”œâ”€â”€ jumia_config.py            # Site-specific scraping configs (selectors, headers)
â”‚
â”œâ”€â”€ level3_automated_ingestion_cycles/
â”‚   â”œâ”€â”€ __init__.py        
â”‚   â”œâ”€â”€ automated_scraper.py       # Carries out the html webscraping
â”‚   â””â”€â”€ README.md                  # You are here
â”‚
â””â”€â”€ render_app.py 
      
```

------------

## ğŸ§  Workflow Logic
1. Ingestion Engines

scraper_engine.py â€” Handles HTML-based extraction via BeautifulSoup and requests.

This returns a standardized DataFrames.  

2. Configuration Layer

categories.json â€” Defines multiple category endpoints for web scraping.
 
config.py â€” Custom rules for scraping (headers, base URLs, selectors).

3. Automation & Scheduling

HTML Scraper: Hosted on Render, triggered automatically by UptimeRobot pings to maintain dataset freshness without relying on paid background workers.
 
```bash 
                |
                v
+-------------------------------+
| ğŸ•¸ï¸ Render + UptimeRobot       |
| Runs periodically or pinged   |
| Executes automated_scraper.py |
| Commits scraper_dataset.csv   |
+-------------------------------+

This produces a CSV dataset in the repo:

level3_automated_ingestiion_cycles/
â”œâ”€â”€ scraper_dataset.csv 
```
---------

## ğŸ› ï¸ Tech Stack
**Component	Purpose**
- Python 3.x	Core automation language
- Requests	HTTP requests for both API + HTML ingestion
- BeautifulSoup (lxml)	HTML parsing
- Pandas	Data transformation, merging, deduplication
- JSON	Configuration for endpoints and API mappings
- render
- flask

----------

## ğŸš€ How It Works

- **Configuration** â€” Define endpoints in categories.json (HTML)  

- **Run Locally (optional)** â€” python level3_automated_ingestion_cycles/automated_scraper.py    
 
- **Automated Mode (default)** â€” The Render cloud (render_app.py) triggers every 24hrs, which then carries out the execution, uptime robot pings the render webservice every 5mins to prevent downtime, after successful process run the commits are then automatically saved.

--------

## ğŸ“ˆ Expected Output

âœ… Unified ingestion pipeline   
âœ… Full automation via Render & Uptime Robot
âœ… Version-controlled, self-sustaining data pipeline
âœ… Scalable architecture for future orchestration (Level 5)

---------- 

## Author: Charles Onokohwomo 

**Project: Data Ingestion Lab (ADIP Series)**
