# âš¡ Level 2 â€” Multi-Page Crawling System  
**Repository:** `data-ingestion-lab`  
**Version:** `v2.0.0`  
**Status:** âœ… Completed  

---

## ğŸš€ Overview  
This level upgrades the **Level 1 Single-Page Crawler** into a **fully functional multi-page scraping system**.  
It automatically navigates through paginated results on **Jumia Nigeria**, collecting laptop product data across all available pages.

The system demonstrates:  
- âœ… Pagination awareness  
- âœ… Clean modular function structure  
- âœ… Resilient error handling  
- âœ… Polite crawling with timed delays  
- âœ… Automated CSV export  

This marks a critical step in transitioning from static scraping to a continuous, scalable ingestion workflow.

---

## âš™ï¸ Core Workflow  

| Stage | Description |
|:------|:-------------|
| **1 â€” Input Specification** | User inputs desired laptop specs (e.g., â€œhp core i5â€). |
| **2 â€” HTML Retrieval** | Script sends HTTP requests to the Jumia laptops category, page by page. |
| **3 â€” Data Extraction** | For each product card: name, price, ratings, and product link are captured. |
| **4 â€” Pagination Loop** | The crawler checks for the â€œNext Pageâ€ button and continues until exhausted. |
| **5 â€” CSV Export** | All results are stored in `sample.csv`. |
| **6 â€” Auto-Refresh Loop** | Every 5 minutes, the script re-runs and refreshes data (demonstrating periodic ingestion). |

---

## ğŸ§© File Structure  

```bash
data-ingestion-lab/
â””â”€â”€ level2_multi_page_crawler/
    â”œâ”€â”€ multi_page_scraper.py
    â”œâ”€â”€ sample.csv
    â””â”€â”€ README.md
```

## ğŸ§  Key Functions
|Function | Purpose
|:------|:-------------|
|fetch_laptop_from_page(soup, selector, search_item) | Extracts laptops from a single HTML page.|
|fetch_all_laptops(base_url, headers, selector, search_item) | Loops through all pages and aggregates data.|
|save_to_csv(laptops, filename) | Saves structured output into a CSV file.|
|main() | Orchestrates the entire flow, manages timing and user input.|

##ğŸ§ª Usage
â–¶ Run Script
python multi_page_scraper.py

ğŸ’¬ Example Prompt
Enter the laptop specification you want to search for: hp core i7

ğŸ“„ Output

sample.csv â€” containing structured columns:

Laptop Name

Price

Ratings

Description Link

ğŸ§± Core Technologies
Library	Purpose
requests	For HTTP requests
BeautifulSoup4 + html5lib	For HTML parsing
csv	For structured data export
time	For crawl delays

## âš–ï¸ Error Handling & Crawl Ethics

- Automatic retries and graceful error handling

- 10-second delay between pages to reduce load

- 5-minute interval between full runs for sustainability

- Configurable headers and base URL for portability

 
## ğŸŒ± Next Milestone â€” Level 3: Modularization

Level 3 will transform this crawler into a modular scraping architecture, introducing:

- core/ â†’ reusable scraping engine

- sites/ â†’ domain-specific configs (selectors, URLs, headers)

- Multi-site ingestion capabilities

- CLI & automation-ready interface

## ğŸ§­ Version History
Version	Update	Description
v1.0.0	Initial Crawler	Single-page extraction working
v2.0.0	Pagination Upgrade	Multi-page support + CSV integration 


## âœï¸ Author

Charles Onokohwomo - Technologist â€¢ Data Scientist â€¢ AI & Systems Engineer 
