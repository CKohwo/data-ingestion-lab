## ğŸ§  Level 3 â€“ Automated Hybrid Ingestion (Scraper + API)

This level marks the evolution from a simple HTML crawler into a hybrid, autonomous ingestion system â€” capable of fetching data from both web sources (HTML scraping) and structured APIs, then intelligently produces a refreshing datasets on a fixed schedule.

The automation pipeline runs every 5 days via GitHub Actions, fetches and delivers data from multiple ingestion layers, and commits updates directly to this repository â€” creating a continuously self-refreshing data engine.

--------

## âš™ï¸ Core Concept

Objective:
Design a self-sustaining ingestion pipeline that intelligently blends web scraping (for data without APIs) and API-based retrieval (for structured or rate-limited data) into a two distinct dataset.

At this stage, the system demonstrates true autonomy and resilience â€” updating itself without manual triggers, managing versioned data persistence, and maintaining a living dataset repository.

This is a key milestone toward a fully orchestrated ingestion engine (Level 4).

----------

## ğŸ§© Project Structure
``` bash
data-ingestion-lab/
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper_engine.py          # Reusable scraping logic (HTML)
â”‚   â”œâ”€â”€ api_engine.py              # Reusable API ingestion logic
â”‚
â”œâ”€â”€ sites/
â”‚   â”œâ”€â”€ categories.json            # Category mapping for scraping endpoints
â”‚   â”œâ”€â”€ api_sources.json           # List of public or structured API endpoints
â”‚   â”œâ”€â”€ jumia_config.py            # Site-specific scraping configs (selectors, headers)
â”‚
â”œâ”€â”€ level3_automated_hybrid_ingestion/
â”‚   â”œâ”€â”€ automated_scraper.py       # Carries out the html webscraping  
â”‚   â”œâ”€â”€ api_ingestor.py            # API ingestion script 
â”‚   â”œâ”€â”€ scraper_dataset.csv        # Auto-generated scraper dataset
â”‚   â”œâ”€â”€ api_dataset.csv            # Auto-saved dataset extracted via Api       
â”‚   â””â”€â”€ README.md                  # You are here
â”‚
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ auto_ingest.yml        # Scheduler (GitHub Actions)
```

------------

## ğŸ§  Workflow Logic
1. Ingestion Engines

scraper_engine.py â€” Handles HTML-based extraction via BeautifulSoup and requests.

api_engine.py â€” Pulls structured data from REST APIs, handling pagination and response normalization.

Both engines return standardized DataFrames.  

2. Configuration Layer

categories.json â€” Defines multiple category endpoints for web scraping.

api_sources.json â€” Lists active API sources (e.g., exchange rates, products, market data).

jumia_config.py â€” Custom rules for scraping (headers, base URLs, selectors).

Example:

{
  "laptops": "https://www.jumia.com.ng/laptops/",
  "gaming": "https://www.jumia.com.ng/gaming/",
  "accessories": "https://www.jumia.com.ng/computing-accessories/"
}
 
3. Automation Layer (GitHub Actions)

Runs every 5 days on a fixed cron schedule.

Executes both ingestion scripts sequentially.

Commits new data to the repository automatically.
 --------
## DATA-FLOW
+--------------------+
| GitHub Actions     |
| (Scheduler: 5 days)|
+--------------------+
           |
+--------------------+
| Automated Ingestor |
|  â”œâ”€â”€ api_engine.py |
|  â””â”€â”€ scraper_engine.py |
+--------------------+
           |
+----------------------+
| Dataset Repository   |
| (CSV auto-updates)   |
+----------------------+

---------

## ğŸ› ï¸ Tech Stack
**Component	Purpose**
- Python 3.x	Core automation language
- Requests	HTTP requests for both API + HTML ingestion
- BeautifulSoup (lxml)	HTML parsing
- Pandas	Data transformation, merging, deduplication
- JSON	Configuration for endpoints and API mappings
- GitHub Actions	CI/CD automation & scheduling

----------

## ğŸš€ How It Works

- **Configuration** â€” Define endpoints in categories.json (HTML) and api_sources.json (API).

- **Run Locally (optional)** â€” python level3_automated_hybrid_ingestion/automated_scraper.py or python level3_automated_hybrid_ingestion/api_ingestor.py
 
- **Automated Mode (default)** â€” GitHub Actions triggers ingestion every 5 days, executing both engines and committing updates automatically.

--------

## ğŸ“ˆ Expected Output

âœ… Unified ingestion pipeline (API + Scraper)
âœ… Two different dataset refreshed automatically
âœ… Full automation via GitHub Actions
âœ… Version-controlled, self-sustaining data pipeline
âœ… Scalable architecture for future orchestration (Level 5)

----------

## ğŸ§­ Next Step: Level 4 â€“ Orchestrated Intelligence

The next level transitions from hybrid ingestion to orchestration and insight automation, where the system doesnâ€™t just collect data â€” it interprets, summarizes, and generates human-readable analytical insights automatically.
-----------

## Author: Charles Onokohwomo 

**Project: Data Ingestion Lab (ADIP Series)**
